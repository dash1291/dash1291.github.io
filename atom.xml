<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Ashish Dubey's Blog</title>
    <updated>2015-09-20T00:00:00Z</updated>
  <id>https://ashishdubey.xyz//</id>
  <link rel="alternate" href="https://ashishdubey.xyz/" title="Ashish Dubey' Blog" type="text/html"/>
	<link rel="self" href="https://ashishdubey.xyz/atom.rss" title="Ashish Dubey's Blog" type="application/atom+xml"/>
  <author><name>Ashish Dubey</name></author>
  
    <entry>
      <id>https://ashishdubey.xyz/2015/09/20/serviceworkers.html</id>
      <title type="text">Using ServiceWorkers for Offline Reading</title>
      <link href="https://ashishdubey.xyz/2015/09/20/serviceworkers.html" rel="alternate" type="text/html"/>
      <updated>2015-09-20T00:00:00Z</updated>
      <published>2015-09-20T00:00:00Z</published>
			<content type="xhtml" xml:base="https://ashishdubey.xyz/">
        <div xmlns="http://www.w3.org/1999/xhtml"><p><em>While I was playing with ServiceWorkers, I got a lot of ideas on how I could use this powerful feature. But making my blog accessible offline seemed to be the easiest and least useless of them.</em></p>

<p>So ServiceWorkers is this new-ish browser feature, still under rapid development, that allows web developers take control of the networking part of the requests they make. Using this, one can write pieces of JavaScript code that can intercept all the requests that their web application makes and take further control of them. A ServiceWorker can essentially act like a proxy server. You can use them for caching your HTTP requests, provide offline experience, selectively routing HTTP requests, etc.</p>

<p>While ServiceWorkers allows you to intercept your HTTP requests, it also provides API for caching your requests. So, you can use these APIs, for populating the ServiceWorker cache, with whatever HTTP requests you want. Once your ServiceWorker intercepts an HTTP request, you can query its cache whether it has the response for that particular request. If you find it, you can return the cached response, otherwise you can propagate the request over the actual network. For creating an offline experience, you would do the same in reverse order: <em>intercept a request -> make the request over the network -> if it succeeds then return the response, else return the cached response</em>. In some network setups, it might take a while before you know the request has failed over the network and you should return the cached response. This might lead to long response times leading to bad user experience. In such cases, you can return a cached response while you send the request over the network and have in place some mechanism to update the content when you get it from the network.</p>

<p>Since I'm too lazy to do the latter, I decided to go for the simpler approach. This page initializes a ServiceWorker for this website. The ServiceWorker, once active, populates its cache with all the pages on this blog (Yes I'm sorry, it just did it or its doing it). For subsequent visits to the other pages, the ServiceWorker will intercept the HTTP request, try to fetch the content from the server. If it fails to do so, it will return the content stored in its cache as the response. Thats it. If you've read this far, and you're not on a dial-up connection, the ServiceWorker must have done its job, and my offline readable blog is your reward.</p>

<p>Wait a minute though. This would work only if you're on Chrome 40 or later. Might also work on Firefox nightly but I didn't check it. Let me know if it works for you there, or if it doesn't work if you expected it.</p>

<p>If you'd like to start playing with ServiceWorkers, this is a nice talk to watch - <a href="https://www.youtube.com/watch?v=SmZ9XcTpMS4">https://www.youtube.com/watch?v=SmZ9XcTpMS4</a>.</p>

<p>If you'd like to know the current status of ServiceWorkers, follow this link - <a href="https://jakearchibald.github.io/isserviceworkerready/">https://jakearchibald.github.io/isserviceworkerready/</a></p>
</div>
      </content>
    </entry>
	
    <entry>
      <id>https://ashishdubey.xyz/2014/12/28/browserstack-infantry.html</id>
      <title type="text">JS, Browsers in Cloud and Parallel Indexing</title>
      <link href="https://ashishdubey.xyz/2014/12/28/browserstack-infantry.html" rel="alternate" type="text/html"/>
      <updated>2014-12-28T00:00:00Z</updated>
      <published>2014-12-28T00:00:00Z</published>
			<content type="xhtml" xml:base="https://ashishdubey.xyz/">
        <div xmlns="http://www.w3.org/1999/xhtml"><p><em>This post is about a weekend hack I put together involving a tiny JavaScript library that I wrote a while ago (I call it "infantry") and using it on BrowserStack for experimentation and fun.</em></p>

<h2>Story of Infantry</h2>

<p>The idea of Infantry came into existence a while ago, when I was reading about techniques used for parallel processing of large datasets. At the same time, I was intrigued by the idea of having a website and distributing your processing power across your visitors' machines. The idea seemed pretty cool to me, so I started my research and found <a href="https://www.igvita.com/2009/03/03/collaborative-map-reduce-in-the-browser/">this cool blog post</a> which meant people had already tried this and it might work. I also learnt about project <a href="http://boinc.berkeley.edu/">BOINC</a> which seemed very impressive to me considering how well they used the concept of collaborative computing and achieve some great results out of it. As a matter of fact, I regularly use BOINC to volunteer some of my idle CPU power hoping that it might be helpful in finding extraterrestrial intelligence someday. Hope is a good thing.</p>

<p>Coming back to Infantry, I really wanted to build a small map-reduce libary in JS that can be used to parallelize simple logic across machines and run inside a web browser. After much procrastination, I decided to start working on it while I was on a 12 hours long flight. I tried to polish the framework further based on Google's map-reduce paper and ended up with something actually working.</p>

<p>The code has always been on GitHub and can be found <a href="https://github.com/dash1291/infantry">here</a>. I have tried to write documentation, but don't expect too much from it, instead just take a look at the example in the repo.</p>

<h2>About the Hack</h2>

<p>For running map-reduce jobs with infantry, one needs to start the infantry server, which acts as master, and to start a worker one has to just navigate to the server's URL inside a browser. Since, BrowserStack allows you start browsers in cloud and test your web pages inside them, it occurred to me to try it as a test bed for Infantry. I wrote a piece of very buggy and fragile code to build an inverted index for a set of HTML pages. After a few minor fixes, I setup the code on my brand new DigitalOcean server, and when I tried to run it, this is the next thing I see:</p>

<p><img src="/static/img/infantry_scr.png" alt="Screenshot" /></p>

<p>Everything worked as expected. Two workers were started on BrowserStack, and infantry was executing the jobs inside both of them, working to count words in a couple of articles from the <a href="http://aosabook.org/en/index.html">AOSA book</a> and build an inverted index.</p>

<p>The code involved can be found <a href="https://github.com/dash1291/browserstack-infantry">here</a>. If I get a chance, maybe I will try to iron out the example a bit, and post some execution stats in the updates. Suggestions are welcome.</p>
</div>
      </content>
    </entry>
	
    <entry>
      <id>https://ashishdubey.xyz/2014/03/22/mozilla-summer.html</id>
      <title type="text">Communication Dashboard in Firefox Marketplace</title>
      <link href="https://ashishdubey.xyz/2014/03/22/mozilla-summer.html" rel="alternate" type="text/html"/>
      <updated>2014-03-22T00:00:00Z</updated>
      <published>2014-03-22T00:00:00Z</published>
			<content type="xhtml" xml:base="https://ashishdubey.xyz/">
        <div xmlns="http://www.w3.org/1999/xhtml"><p><em>This is one of the awfully long-time overdue posts I've had in my mind, but have not been able to make it on this space. But since its never too late, I'm writing about the cool project I was working on while interning at Mozilla during the summer of 2013.</em></p>

<h2>About Marketplace</h2>

<p><a href="https://marketplace.firefox.com">Firefox Marketplace</a>, for the uninitiated is the platform where developers can host their Firefox apps which run on desktop and mobile (of course including Firefox OS). It acts as the central repository of apps, from where users can download and install apps on their systems. And by that I don't mean a plain repository of apps which just allows you to download apps, but also essentials like compatibility checks, app analytics, ratings, reviews, etc. Just like <a href="https://addons.mozilla.org">Firefox Add-ons</a>, there are apps for various kinds of users on Marketplace. Since Mozilla cares much about the quality of the apps served to its users, it makes sure that the apps meet a quality standard before they are available on the Marketplace. For this purpose, there is process of App Review which comes after a new app is submitted to Firefox Marketplace and before it is available to the users for download. App reviewers, which include paid staff and volunteers, take a close look at the submitted apps, beyond just the description of it. It includes inspecting the app's functionality, performance, and security. If the reviewers like it, it goes live for the users to download, if not, the developer (who submits the app) is notified about the issues found during the review, and the developer is expected to fix those before it can be reviewed again and made public.</p>

<h2>About the Problem</h2>

<p>So, in the Marketplace workflow, we have three kinds of users namely, developers, reviewers and users (Firefox users who download the apps). The review process is basically hidden from the users as it is not relevant to them. Its the developers and reviewers which interact with each other during the review process mainly for feedback, reporting issues and asking questions. The Reviewer Tools provides a dashboard for the reviewers to review apps, which allows them to pick apps they want to review, and take actions on the apps they are reviewing (that is reject, make public, delete or ask for more information). On the other hand, Developer Hub is the dashboard for the developers, which they can manage apps (that is upload an app, edit uploaded apps). Though, this is an efficient workflow for both developers and reviewers, but on the communication front they are a bit disconnected. Every time, a reviewer takes an action on an app, its developer is notified about the action and the enclosed comments in an email. If the developer has any question or comment on the action, they reply back to the email which is delivered to a mailing list which has all the reviewers as its members. While the process works and is transparent, its not the best experience for both developers and especially reviewers. There is no central place in Marketplace where users can view all their conversation. It is totally tied up with their email. Also, communication on mailing lists gets messy after a while, reviewers have to keep track of developers' email addresses, among other hassles. So my project for the summer was to revamp this process in the following ways:</p>

<ol>
<li><p>Create a dashboard for both reviewers and developers to access their present and past conversations. This should be used to read and reply to the messages exchanged between the users.</p></li>
<li><p>Bridge the email conversation and dashboard, so that the users can use both modes of communication as they please.</p></li>
</ol>

<h2>About the Solution</h2>

<p>This is what a part of solution looked like:</p>

<p><img src="/static/img/commbadgescreen.png" alt="Dashboard Screenshot" /></p>

<p>This is the communication dashboard frontend which is written over the same codebase that the Firefox Marketplace is based on, that is <a href="https://github.com/mozilla/fireplace">fireplace</a>, which is essentially a homegrown client-side framework with Marketplace specific bits like how the requests are dispatched, responses are cached, and rendered. Later, most of the Marketplacey commonware was abstracted out and <a href="https://github.com/mozilla/commonplace">commonplace</a> was born, which served as the base for all other Marketplace related projects, including communication dashboard frontend, source code of which resides in <a href="https://github.com/mozilla/commbadge">commbadge</a>.</p>

<h2>More About the Solution</h2>

<p>A little about the frontend was described above, but that was only a part of the entire thing. The frontend is basically a client-side app which talks to the Marketplace REST API, for reading/writing communication dashboard data. This is actually how Firefox Marketplace works. The frontend is a client-side app which fetches data from the Marketplace API and renders it using <a href="http://jlongster.github.io/nunjucks">Nunjucks</a> templating library. The Marketplace API codebase is written in Python/Django (codename <a href="https://github.com/mozilla/zamboni">zamboni</a>), and is huge and might require an additional blog post to get into the details. For the purpose of this post, lets just stick to the details of Marketplace API relevant to the communication dashboard API.</p>

<p>The REST API is written using the <a href="http://django-rest-framework.org">Django REST Framework</a>, which makes it really easy and quick for writing new API endpoints, once you understand how it works. Since, I had worked on Tastypie before, it took me a while to understand different parts of DRF work together, but spending time reading the documentation and codebase itself, yielded good results. I realized that DRF is infact, very simple and flexible framework, making it really easy to control different parts of an API request lifecycle in DRF, simply by extending different classes that DRF provides. I've actually used DRF for another personal project of mine, and it helped setup an API in no time.</p>

<p>One requirement of the project, is to be able to handle the incoming email replies. This means that, a reply to a message sent from communication dashboard can be sent over email and the replies should be tied to the same conversation thread. This is done by sending an email for every message that is sent from the communication dashboard, to the recipients of the message through email. A unique reply token is embedded in the <em>Reply-To</em> address of each email, which internally corresponds to a single conversation thread. Whenever a reply is sent to the reply address, the email server pipes the email to the communication dashboard API, which in background then extracts this reply token and figures out which thread it belongs to and creates its record in the database. Works pretty neatly.</p>

<h2>About the Result</h2>

<p>Much of the prototyping was done by the time I was done with my internship, but the project could not get deployed, which kept me from addressing the real issues it might face in production. But it was a fun ride. It was challenging to find edge cases, and dozen of issues which would arise in every single bug that was part of the project.</p>

<p>My project presentation can be viewed at <a href="https://air.mozilla.org/2013-intern-dubey">air.mozilla.org/2013-intern-dubey</a>, slides of which are available <a href="http://slid.es/ashishdubey/internpresentation13">here</a>.</p>

<p>In a nutshell, I had an awesome summer!</p>

<h2>Thanks</h2>

<p>The awesome summer would not be possible and much fun without my supervisor <a href="https://twitter.com/cvanw">cvan</a>. Learnt so much from him and <a href="https://twitter.com/mattbasta">Matt Basta</a> while working on commbadge, commonplace and zamboni, having fun code reviews. <a href="https://twitter.com/clouserw">Wil Clouser</a> and the entire Marketplace team for a place in the awesome team.</p>

<p>Special thanks to <a href="http://ngokevin.com">Kevin Ngo</a> for taking forward the work on communication dashboard, making it awesome and suck less.</p>
</div>
      </content>
    </entry>
	
    <entry>
      <id>https://ashishdubey.xyz/2012/11/9/crunch-in-network.html</id>
      <title type="text">Putting Some Crunch in the Network</title>
      <link href="https://ashishdubey.xyz/2012/11/9/crunch-in-network.html" rel="alternate" type="text/html"/>
      <updated>2012-11-09T00:00:00Z</updated>
      <published>2012-11-09T00:00:00Z</published>
			<content type="xhtml" xml:base="https://ashishdubey.xyz/">
        <div xmlns="http://www.w3.org/1999/xhtml"><p>Most of us, who write web pages, sometimes want to serve their content from the local server. Now if I'm one of the students of a college, where there are no ports forwarded on which I'm allowed to serve my content, being on the college's network. Thats bad because I often want to be able to serve some static pages or files from my local system to the internet. I wish there was no firewall but there are somethings you cannot change and have to live with. But certainly, one can adapt. I've made a small attempt towards such an adaptation.</p>

<h2>What about adaptation?</h2>

<p>I started writing some experimental tools which would allow me to serve content from any such network where I cannot serve directly to the internet. All I require is just one port that allows me to make outbound remote connections from my network and a remote server, which should be under my control. I need to run a python program on my local system and another on my remote server and I get a bypassing highway for my content(which is on my local system) to reach where it is needed. Its really that simple, but works, hence very useful.</p>

<h2>Enter Crunch</h2>

<p>So the tools consist of a server and client, which runs a serving node on your local machine. Lets call these serving nodes as crunch nodes. The server receives HTTP requests from the internet, and according to URI of the request, pulls content from a crunch node. Now an important thing to note here is that the crunch nodes create persistent connections with the server. They initialize these connections with the server, so the need to accept connections on crunch nodes is eliminated.</p>

<p>The server and the crunch nodes communicate over a very basic TCP protocol(something like IRC). Since, it has just begun yet, there are limits to serving. It can only serve static files. Some HTML, CSS, JS, or any other file. Moreover, its also not fit for serving large files yet. I've seen it break while serving JPEGs of sizes like 200KBs. Its an issue, but not a big one, so it will be fixed soon. Apart from that, I've planned so many enhancements to it. Perhaps, too many to list here. To put it simply and in short, I would love to make it fit for serving any pretty thing someone would want to serve from a local system, even dynamic web content. But, thats long way ahead.</p>

<h2>Usage</h2>

<p>It does not make sense for such a thing to exist if you can't use it right? So lets squeeze out a simple feature of serving a small text file with crunch.</p>

<p>Install crunch:</p>

<pre class="bash">
$ pip install -e git+https://github.com/dash1291/crunch.git#egg=crunch
</pre>

<p>Write your server:</p>

<pre class="python">
from crunch.server import runserver

runserver(8000, 8001)
</pre>

<p>When you run the above code, your server will start accepting HTTP requests on port 8000 and will accept connections from crunch nodes on port 8001. You have the liberty to choose those ports. As a general rule, the first port(HTTP) can be anything because on your remote server, you can accept connections on most of the ports. But the second port can only be a port which is open on your network, such that it allows outbound remote connections. For example, in my own setup, I use port 80 as the HTTP port, and 21 as the crunch port, because 21 is one of the few ports that I can use to make outbound remote connections. This could work for you or you might need to alter it a bit to make it work for you.</p>

<p>We will need to create database schema for the users and atleast one user account to be able to connect to the server. So we'll do that from the Python shell.</p>

<pre class="pycon">
&gt;&gt;&gt; from crunch.crunchauth import add_account, create_schema
&gt;&gt;&gt; create_schema()
&gt;&gt;&gt; add_account('someone', 'someonespassword')
</pre>

<p>Write your crunch client:</p>

<pre class="python">
from crunch.crunchclient import start_client

config = {
    'username': 'someone',
    'password': 'someonespassword'
    'address': 'something.com',
    'port': 8001
}

start_client(config)
</pre>

<p>When you run this code, you will be connected to the crunch server. Remember to put in the right values to the config fields. When its running, whenever, the server receives an HTTP request at <em>/someone/hello.txt</em>, the crunch client will look for the file <em>hello.txt</em> in the current directory and if found, it will send it to the server which is written back as a response to the HTTP request.</p>

<p>Lets put it to test, assuming there is a <em>hello.txt</em> in the current directory which contains <em>Hello World</em>.</p>

<pre class="bash">
$ curl http://something.com:8000/someone/hello.txt
Hello World
</pre>

<p>So thats it. One last point to note here is, the project is open for hacking. If you are over just suggestions, you are welcome to pitch in some code and make it better yourself. The code can be found on <a href="https://github.com/dash1291/crunch">GitHub</a>. Use it or fork it, if you want to make it better, and put some crunch in your network.</p>
</div>
      </content>
    </entry>
	
    <entry>
      <id>https://ashishdubey.xyz/2012/7/5/realtimeve-first-demo.html</id>
      <title type="text">Collaborative VisualEditor: First Demo</title>
      <link href="https://ashishdubey.xyz/2012/7/5/realtimeve-first-demo.html" rel="alternate" type="text/html"/>
      <updated>2012-07-05T00:00:00Z</updated>
      <published>2012-07-05T00:00:00Z</published>
			<content type="xhtml" xml:base="https://ashishdubey.xyz/">
        <div xmlns="http://www.w3.org/1999/xhtml"><p>I feel pretty bad for not having written about my Google Summer of Code project that I'm undertaking this summer for Wikimedia Foundation for prototyping collaborative editing feature for the new under-development VisualEditor(wysiwyg). Its been a while since I was selected, and started working on it, so I won't go into discussing the story around how I got started on it. But, I would like to discuss about the present state of the project, where its heading and also a bare working demo of the code produced so far.</p>

<p>So the VisualEditor project is a wysiwyg editor for MediaWiki, designed from scratch keeping in mind the complexities of the Wikitext mapping to HTML and data structures suitable for inline editing. The changes to the text in VisualEditor are in the form of transactions. There are transactions for insertion/deletion of text as well as applying formatting attributes to a portion of text. 
So most of the collaboration thing revolves around these transactions. There is a collaboration server which listens for new connections from clients who requests to initiate an editing session. A client module establishes connection with the server, and listens to the changes made in the document and translates the resulting transactions to the server. A thing that should be noted here is, the project is under Phase 1 which would support only one client who publishes changes to the server's document and other connected clients simply listen to these changes and apply them in their local documents. Phase 2 would support multiple publishers which would bring in conflicts and concurrency issues and perhaps measures to fight with them.</p>

<p>So, there is a small list of things which have been done so far -</p>

<h2>On the Server</h2>

<ol>
<li>Listen to new connections and define I/O events.</li>
<li>Bind the VE modules into a single module which is imported as a top-level <code>ve</code> object.</li>
<li>Create a new user session on every new connection and associate with the requested document.</li>
<li>Create a new document model instance if an editing session is initiated on a new document.</li>
<li>Apply incoming transactions to the document model.</li>
<li>Invoke/revoke publishing rights on user sessions.</li>
</ol>

<h2>On the Client</h2>

<ol>
<li>Initialize a new session with the server on manual request.</li>
<li>Enable/Disable editing based on publishing right flag received from the server.</li>
<li>Listen to document change events and push transactions to the server if publishing is given.</li>
<li>Apply incoming transaction from the server if the origin of the transaction is a different client.</li>
<li>Retain the document state before collaborative editing is turned on so it can be restored when collaborative mode is turned off.</li>
</ol>

<h2>Brief Internals</h2>

<p>The collaboration server is a Node.js based server, which uses socket.io for making persistent connections with the clients for realtime communication. The client module binds against the server's API which is laid in terms of the socket.io events defined in the server. All the VE functionality is accessed through a binding module which exports a top-level <code>ve</code> object much like the <code>ve</code> namespace is used on the client-side. The server can parse the wiki pages internally using the parser modules and also can fetch the parsed HTML output from an external parsoid service over HTTP. As of now, an external parsoid service is used for scalability until something better can be figured out.</p>

<h2>What Should be Coming?</h2>

<ol>
<li><p>Authentication</p>

<blockquote>
  <p>Some code has been written for handling user authentication on the socket connections level but is not functional right now. Also, due some review on the proper technique to do it, its deferred for a little later.</p>
</blockquote></li>
<li><p>Editing control transfer</p>

<blockquote>
  <p>Right now editing control is given to any user who joins when there is no client with publishing right is connected to the server on a same document. A publishing user before disconnecting might want to transfer the editing control to a client which is already connected as non-publishing client.</p>
</blockquote></li>
<li><p>Transactions buffering</p>

<blockquote>
  <p>Right now every transaction is fired towards the server as it is generated in the VisualEditor. This would be quite a network activity. So, there would be some buffering and capping of the transactions before they are transported to the server.</p>
</blockquote></li>
</ol>

<h2>About the Demo</h2>

<ol>
<li>Updated fork of <code>realtimeve</code> branch of VisualEditor.</li>
<li>Runs the collaboration server alongside the parsoid service in the same master process but on a different port. The collaboration server uses this parsoid service to fetch parsed HTML over HTTP.</li>
<li>The client UI lets users to turn-on/off the collaborative editing mode.</li>
<li>The user with the editing control is shown in green in the users list displayed on the right.</li>
</ol>

<p><a href="http://ashishdubey.info/mw/index.php"><strong>Link to the demo site</strong></a></p>

<h3>Less Important (Fancy) Information</h3>

<p>The demo is hosted on an Amazon EC2 micro instance, running Ubuntu 11.10. The server uses Node.js v0.8.1 and socket.io v0.9.6. The cluster module used in running the parsoid service forks five workers while running the collaboration server on the master process.</p>

<h2>Reporting issues</h2>

<p>If you've got suggestion to improve a feature, or about a new feature or you want to report some bugs, you can get in touch with me directly through my email - ashish[dot]dubey91[at]gmail[dot]com.</p>
</div>
      </content>
    </entry>
	
    <entry>
      <id>https://ashishdubey.xyz/2012/4/17/opensource-wowness.html</id>
      <title type="text">Open source Wowness</title>
      <link href="https://ashishdubey.xyz/2012/4/17/opensource-wowness.html" rel="alternate" type="text/html"/>
      <updated>2012-04-17T00:00:00Z</updated>
      <published>2012-04-17T00:00:00Z</published>
			<content type="xhtml" xml:base="https://ashishdubey.xyz/">
        <div xmlns="http://www.w3.org/1999/xhtml"><p>This is the first in set of the delayed blog posts that I wanted to write since long but somehow did not. After the awesome December dedicated to Python and Mozilla, and with Mozilla's AMO project being the first open source project that I contributed to, I easily understood the joy and learning that comes through contributing to an awesome open source project. And with that motivation, and mostly with a desire of contributing to another open source project I turned to <a href="http://mediawiki.org">MediaWiki</a>. I was definitely not here for PHP parts of it but still to get a hang of the software I developed a small experimental extension for it. It was a smooth run, so I continued to discover more until I discovered the project that I was maybe looking for, that is, the <a href="http://mediawiki.org/wiki/Visual_editor">Visual Editor project</a>. This looked to be a really cool project at first, because I've had some tough time editing wiki pages for the first time using the wiki markup text. But the Visual Editor project simply appeared to solve the wiki editing problems for ever and everyone, and so was it. Mostly with the desire to make some good amount of contribution to MediaWiki through GSoC, I started hacking on the Visual Editor project with an aim of prototyping the collaborative editing feature on Visual Editor. With hacking came the learning, mainly on numerous pro JS practices, so I was enjoying it! I talked to alot of devs mainly those onboard the VE team. They told me what made sense and what not in what I was aiming.</p>

<p>By this time, I really wished to be the part of this community. I attended a hackathon in Pune in February which happened to be a great event where I worked on Wikipedia's official mobile app and met some awesome people from Wikimedia Foundation. And this is when I realized how awesome the Wiki community is and felt the desire to make some considerable contribution to it.</p>

<p>A unique thing about contributing to Wikimedia projects is, your work reaches people who use so many different languages which is achieved better in no other open source project. And that is the joy a developer seeks while contributing to any open source project.</p>
</div>
      </content>
    </entry>
	
    <entry>
      <id>https://ashishdubey.xyz/2012/1/5/december-of-things.html</id>
      <title type="text">December of Things</title>
      <link href="https://ashishdubey.xyz/2012/1/5/december-of-things.html" rel="alternate" type="text/html"/>
      <updated>2012-01-05T00:00:00Z</updated>
      <published>2012-01-05T00:00:00Z</published>
			<content type="xhtml" xml:base="https://ashishdubey.xyz/">
        <div xmlns="http://www.w3.org/1999/xhtml"><p>I'm just done with a month long winter break after the semester finished at
school. The month of December was about some travel, lot of sleep, occasional
movie sprints, and lastly but most importantly it was about taking my
experiences with Python some steps forward.The list of experiments included a
handful with Twisted including an incomplete(+ abandoned) networking project,
gitpython, a small screen grabber(+ uploader) script and a couple of patches
contributed to one of the Mozilla.org websites that is
<a href="http://addons.mozilla.org">addons.mozilla.org</a> a.k.a. AMO. These experiments
though not involving coding a huge project all the way, did well in teaching
me some really important things both at fundamental levels and sometimes
arbitrary magic tricks.</p>

<h2>Recap. Me Earlier with Python.</h2>

<p>I started learning Python about 4 months back in September, last year. And it
was this time, that I realized that Python while being easy and quick to learn
can be very useful. I did my first Python work towards a special task which
was creating a script that runs as a daemon on my shared web host, crawls for
artists on Last.fm, pull their biographical information and look for the
location terms using Yahoo Placemaker API and eventually determine the
geographical location to which the artist belonged. I needed such a bot script
to populate a database of artists with their geographical locations for my
website project which was about music discovery over geographical data. My
shared host supported Python, I heard Python would be good for making such
scripts, and I wanted to learn Python so it was an obvious to choose Python
for the script. Being a PHP programmer, I used analogies to PHP methods of
parsing the web documents, and random smaller stuff to make a quick leap to
learning Python while directly coding my script. It was a smooth run, much
much smoother than I had expected and apart from some nasty bugs which broke
the script sometimes, it worked really good. Good enough to give me enough
artists to get the website live within 3 days after starting the script on my
shared web host. </p>

<p><strong>Enclosed Lessons</strong></p>

<ol>
<li>Python has everything that PHP has. urllib, xml_minidom made me feel home.</li>
<li>Life's easier without braces.</li>
</ol>

<p><strong>Some Links</strong></p>

<ul>
<li>Checkout the website. <a href="http://geonres.com">Geonres.com</a> </li>
<li>Checkout the source of the script on <a href="https://github.com/dash1291/geonres_bot">GitHub</a>. </li>
<li>Checkout the website source on <a href="https://github.com/dash1291/geonres.com">GitHub</a>. </li>
</ul>

<h2>Exposure to the Community. PyCon India 2011</h2>

<p>I registered for PyCon India before actually starting to work with Python
which was because of my desire to learn the awesome language. I was really
excited as it was the first time that I was going to attend such a large
community meetup. Being interested in web development, I attended every
talk/session that gave me a hint of something that I could use to build web
applications using Python notably including some like the low level web-
scrapping tutorial, a talk on Redis, and another one on pyjamas. They came
with great learning and I got to know people who've been doing great things
with the awesome language that I was so wanted to learn. All that was a great
motivation package to keep up and working. </p>

<p><strong>Enclosed Lessons</strong></p>

<ol>
<li>One can do pretty much anything, perhaps more things than any other language using Python, in ways that are easier and quicker.</li>
<li>Python has an awesome community full of cool people who do cool stuff every day, every night and in every part of the globe.</li>
</ol>

<h2>Trying out things with Web Development with Python. Learning Django.</h2>

<p>When I started learning Python, I knew I wanted to use it as one my tools for
developing web applications along with PHP. I heard the names of some web
frameworks built in Python like Django, Google's webapp, pylons, and web2py.
When I visited PyCon India, I heard more about them and much more about Django
and how much better it is than any of the Python web frameworks out there, or
perhaps better than many web frameworks built in different languages too. I
wanted to give it a wild try to learning it. I usually have a tendency to
learn while doing, but I didn't have an idea of an application to start with
so I though I would remake a web application that I earlier developed using
PHP(I called it shitstream), but this time in Django. Since, it was my first
attempt to any MVC framework, shitstream appeared to be a good choice to get
some basic concepts built up, as the app was simple and compact. </p>

<p><strong>Enclosed Lessons</strong></p>

<ol>
<li>Django is the best thing one can use to build web applications.</li>
<li>Unless I'm out of choices or luck, I would never go back to PHP again.</li>
</ol>

<p>Checkout the source of Shitstream app on
<a href="https://github.com/dash1291/shitstream">GitHub</a>.</p>

<h2>Helping make Web Better. Fixing @ <a href="http://addons.mozilla.org">addons.mozilla.org</a></h2>

<p>I wanted to contribute to Mozilla in any way possible since the day I got to
know about Open Source Software, about Mozilla, and how Mozilla keeps doing
awesome things which make the web much more awesome than it was, or perhaps
much more awesome than how it is now. But, with a huge rack of Mozilla
projects to choose from, my unawareness about them, and my lack of focus in a
single field kept me confused whenever I wanted to give a try to contributing
to one of the projects at Mozilla. But when I learnt Django, and accidentally
browsing through the Web Dev projects at Mozilla's Contribution page, I found
my thing. Having developed two Firefox addons earlier, I was aware of AMO, or
I should say that I hung-out quite alot on that website. And, AMO being the
first in the web dev projects list, I could not wait to browse through the
bugs list and kick out a few of those annoying bugs. It did not take me long
to fork the Mozilla's github repository of Zamboni, clone it to my local box,
and start trying to set it up. But setting up the whole thing, that is my
local AMO instance took me 3 days. Yes, 3 days! It was actually a pain setting
up the huge stack on my Ubuntu system. Thanks to folks on IRC, who are patient
great helpers and played angels in helping me setting it up especially Jeff
Balogh. Not only setup, but he also guided me to how to start fixing my first
bug and led me all the way to my first pull request, which was much required
for me as it was not only my first attempt to fix a bug on this project, but
also my first attempt to fix anything on any open source project. My first
pull request although took a while to be closed with a merge of my proposed
patch because it was awaiting review from the reporter and in the meantime I
was also busy with college. But as the winter break started, I got it merged,
and also worked to fix a couple more bugs and got my patches merged. All the
while I was working on these bugs, Mozilla developers hanging out on IRC were
really helpful while being prompt at suggestive code reviews especially Chris
Van, who is also the commiter of all my patches to the original AMO git
repository. </p>

<p><strong>Enclosed Lessons</strong></p>

<ol>
<li>Most importantly, I learnt how to write code that is readable and stylish. Thanks to Mozilla's adoption of PEP8.</li>
<li>Ample of git stuff, I knew a very limited subset of git before starting to work on this project. I'm still terrible, but much better than what I was earlier.</li>
<li>git rebase can be dangerous while working with a lot of people.</li>
<li>find . -name "*.pyc" -delete is simple but can be magical sometimes.</li>
</ol>

<p>Have a look at my commits merged with AMO
<a href="https://github.com/mozilla/zamboni/commits/master?author=dash1291">here</a>. So
that was pretty much my Python story so far, and another of my favorite
December. What I've figured out so far is that Python and Mozilla communities
are two of the best Open Source Communities working to promote openness
everywhere and Mozilla doing their every bit to build an open web platform
which is not just about Mozilla Firefox but freedom and choice to the users of
web around the globe.</p>
</div>
      </content>
    </entry>
	
    <entry>
      <id>https://ashishdubey.xyz/2011/10/29/django-apache-modwsgi.html</id>
      <title type="text">Setting up Apache Server for Django with mod_wsgi</title>
      <link href="https://ashishdubey.xyz/2011/10/29/django-apache-modwsgi.html" rel="alternate" type="text/html"/>
      <updated>2011-10-29T00:00:00Z</updated>
      <published>2011-10-29T00:00:00Z</published>
			<content type="xhtml" xml:base="https://ashishdubey.xyz/">
        <div xmlns="http://www.w3.org/1999/xhtml"><p>Django is a very cool web development framework which comes with its own
lightweight development server which is best for testing the application. But
when it comes to deploying the application for the outer world, more efficient
and secure web server like Apache is preferred. Now, web developers who have
been developing applications using PHP might find it very easy to deploy their
applications on Apache(as they have easy configurable PHP module, or they
could just find a LAMP stack for if they were on Linux, so they would get
everything cooked. But when it comes to combining python web frameworks like
Django with Apache, things turn out to be less obvious. My experience weren't
smooth at all while trying to get my Django project work on Apache. And since
I was trying to configure it with already installed LAMP stack on my Ubuntu
system, things went really bad. Since, there are so many things integrated in
a compilation like LAMP stack, there are as many things which can go wrong
while configuring Django on an Apache server. So, its recommended that a clean
and separate Apache installation should be used so that things are easier to
diagnose one by one.</p>

<h2>Step 1 - Install Apache HTTP Server</h2>

<p>You should install a clean build of Apache in case you haven't already. Its
easy, just follow the docs given here <a href="http://httpd.apache.org/docs/2.2/install.html">http://httpd.apache.org/docs/2.2/instal
l.html</a></p>

<h2>Step 2 - Install mod_wsgi</h2>

<p>Now that you're done with Apache, you need to install mod_wsgi which is a WSGI
module for Apache. Know more about it here.
<a href="http://code.google.com/p/modwsgi/">http://code.google.com/p/modwsgi/</a></p>

<p>Installation instructions <a href="http://code.google.com/p/modwsgi/wiki/InstallationOnLinux">http://code.google.com/p/modwsgi/wiki/InstallationO
nLinux</a> </p>

<h2>Step 3 - Load and Configure mod_wsgi in Apache config</h2>

<p>This is where you edit the Apache
configuration file and tell Apache to load mod_wsgi module using the DSO
method. You need to edit /conf/httpd.conf file and add the following directive
to it</p>

<pre class="apache">
LoadModule wsgi_module /usr/lib/apache2/modules/mod_wsgi.so
</pre>

<p>Also, at the end of the httpd.conf file, add the following directives</p>

<pre class="apache">
WSGIScriptAlias / &lt;path-to-your-django-project&gt;/test.wsgi
&lt;Directory path-to-your-django-project&gt;
Order deny,allow
Allow from all
&lt;/Directory&gt;
</pre>

<p>The first line in the above directives loads mod_wsgi module into the Apache
instance. The second directive <Directory> directive is there to make sure
Apache can access the directory given in the path. Also, make sure you replace
<path-to-your-django-project> with its appropriate value.</p>

<h2>Step 4 - Test WSGI</h2>

<p>Now that we've configured Apache to load the mod_wsgi module, its time to test
if it works. To do that, create a file <path-to-your-django-project>/test.wsgi
with the following content.</p>

<pre class="python">
def application(environ, start_response):
    status = '200 OK'
    output = 'Hello World!'
    response_headers = [('Content-type', 'text/plain'),
    ('Content-Length', str(len(output)))]
    start_response(status, response_headers)
    return [output]
</pre>

<p>Now test if it works by trying http://localhost in your browser, you should
get Hello World for obvious reasons.(and yes make sure Apache is running)</p>

<h2>Step 5 - Test the bad WSGI</h2>

<p>Now, that the simple WSGI application runs, we need to test something which
generally has issues which need to be resolved before moving on to using
Django as a WSGI application.</p>

<h3><strong>The Expat woe</strong></h3>

<p>When we move to more complicated WSGI applications, and as the applications
start using libraries whose versions conflict with the versions used by
Apache, there are issues like Apache crashing and incorrect response. One such
library is libexpat. Know more about it here <a href="http://expat.sourceforge.net/">http://expat.sourceforge.net/</a>.</p>

<p>It happens in many situations that the expat module used by python(that is the
default one on a system) has a different version than that used by Apache(it
has its own). In such a situation, there is Segmentation Fault encountered by
Apache and it crashes. To test it if you encounter that issue, just make the
following entry in your test.wsgi file.</p>

<pre class="python">
import pyexpat
</pre>

<p>Now, again try http://localhost. If you get proper response as earlier then
you are free from the expat issue while if you get 'no response' error then
you need to resolve the expat issue before turning on to Django.</p>

<h3><strong>Resolving Expat Issue</strong></h3>

<p>There is this great post on handling expat issue. <a href="http://code.google.com/p/modwsgi/wiki/Issues
WithExpatLibrary">http://code.google.com/p/mod
wsgi/wiki/IssuesWithExpatLibrary</a>.</p>

<p>Since, python's expat module is hard to replace, its
recommended you change Apache's expat module. Either update it or simply
replace it with your system's expat. I consider replacing it. After all done,
restart Apache and test http://localhost, if all went good it should be
working fine.</p>

<h2>Step 6 - Configure WSGI for Django</h2>

<p>If you've successfully dealt with running WSGI with expat loaded, you should
then be able to run Django after a little bit of work if not-so-common issues
don't fall in your way. For connecting to Django with mod_wsgi, you should
create a new django.wsgi in the same directory as test.wsgi and set
WSGIScriptAlias to that very file. Enter the following code to django.wsgi and
you're on the go.</p>

<pre class="python">
import os
import sys
root = os.path.join(os.path.dirname(__file__),'..')

sys.path.insert(0,root)
os.environ['DJANGO_SETTINGS_MODULE']='&lt;your-project&gt;.settings'

import django.core.handlers.wsgi
application = django.core.handlers.wsgi.WSGIHandler()
</pre>

<p>Replace <code>&lt;your-project&gt;</code> with your project's name. </p>

<h2>Step 7 - Test Django</h2>

<p>So you've worked out all the way to the point where you would need to test if all
your setup works fine. Its recommended that you use a clean Django application
so that you don't encounter issues related to database and other stuff.</p>
</div>
      </content>
    </entry>
	
</feed>